{
  "_from": "json2csv",
  "_id": "json2csv@5.0.6",
  "_inBundle": false,
  "_integrity": "sha512-0/4Lv6IenJV0qj2oBdgPIAmFiKKnh8qh7bmLFJ+/ZZHLjSeiL3fKKGX3UryvKPbxFbhV+JcYo9KUC19GJ/Z/4A==",
  "_location": "/json2csv",
  "_phantomChildren": {},
  "_requested": {
    "escapedName": "json2csv",
    "fetchSpec": "latest",
    "name": "json2csv",
    "raw": "json2csv",
    "rawSpec": "",
    "registry": true,
    "saveSpec": null,
    "type": "tag"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/json2csv/-/json2csv-5.0.6.tgz",
  "_shasum": "590e0e1b9579e59baa53bda0c0d840f4d8009687",
  "_spec": "json2csv",
  "_where": "/home/sunilkumar/Documents/onlineBook/sampleProject/SFTP",
  "author": {
    "email": "mirco.zeiss@gmail.com",
    "name": "Mirco Zeiss"
  },
  "bin": {
    "json2csv": "bin/json2csv.js"
  },
  "browser": "dist/json2csv.umd.js",
  "bugs": {
    "url": "https://github.com/zemirco/json2csv/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "commander": "^6.1.0",
    "jsonparse": "^1.3.1",
    "lodash.get": "^4.4.2"
  },
  "deprecated": false,
  "description": "Convert JSON to CSV",
  "devDependencies": {
    "@babel/core": "^7.3.3",
    "@babel/preset-env": "^7.3.1",
    "coveralls": "^3.0.3",
    "docpress": "^0.8.0",
    "eslint": "^6.1.0",
    "gh-pages": "^2.0.1",
    "in-publish": "^2.0.0",
    "nyc": "^14.1.1",
    "rollup": "^1.11.0",
    "rollup-plugin-babel": "^4.3.2",
    "rollup-plugin-commonjs": "^10.0.2",
    "rollup-plugin-node-builtins": "^2.1.2",
    "rollup-plugin-node-globals": "^1.2.1",
    "rollup-plugin-node-resolve": "^5.2.0",
    "standard-version": "^8.0.1",
    "tap-spec": "^5.0.0",
    "tape": "^4.10.1"
  },
  "engines": {
    "node": ">= 10",
    "npm": ">= 6.13.0"
  },
  "homepage": "http://zemirco.github.io/json2csv",
  "keywords": [
    "convert",
    "csv",
    "export",
    "json",
    "parse",
    "to"
  ],
  "license": "MIT",
  "main": "lib/json2csv.js",
  "module": "dist/json2csv.esm.js",
  "name": "json2csv",
  "optionalDependencies": {},
  "readme": "# json2csv\n\nConverts json into csv with column titles and proper line endings.  \nCan be used as a module and from the command line.\n\n[![npm version][npm-badge]][npm-badge-url]\n[![Build Status][travis-badge]][travis-badge-url]\n[![Coverage Status][coveralls-badge]][coveralls-badge-url]\n\nSee the [CHANGELOG] for details about the latest release.\n\n## Features\n\n- Fast and lightweight\n- Scalable to infinitely large datasets (using stream processing)\n- Support for standard JSON as well as NDJSON\n- Advanced data selection (automatic field discovery, underscore-like selectors, custom data getters, default values for missing fields, transforms, etc.)\n- Highly customizable (supportting custom quotation marks, delimiters, eol values, etc.)\n- Automatic escaping (preserving new lines, quotes, etc. in them)\n- Optional headers\n- Unicode encoding support\n- Pretty printing in table format to stdout\n\n## How to install\n\nYou can install json2csv as a dependency using NPM.  \nRequires **Node v10** or higher.\n\n```sh\n# Global so it can be called from anywhere\n$ npm install -g json2csv\n# or as a dependency of a project\n$ npm install json2csv --save\n```\n\nAlso, if you are loading json2csv directly to the browser you can pull it directly from the CDN.\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/json2csv\"></script>\n```\n\nBy default, the above script will get the latest release of json2csv. You can also specify a specific version:\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/json2csv@4.2.1\"></script>\n```\n\n## Command Line Interface\n\n`json2csv` can be called from the command line if installed globally (using the `-g` flag).\n\n```bash\nUsage: json2csv [options]\n\nOptions:\n  -V, --version                       output the version number\n  -i, --input <input>                 Path and name of the incoming json file. Defaults to stdin.\n  -o, --output <output>               Path and name of the resulting csv file. Defaults to stdout.\n  -c, --config <path>                 Specify a file with a valid JSON configuration.\n  -n, --ndjson                        Treat the input as NewLine-Delimited JSON.\n  -s, --no-streaming                  Process the whole JSON array in memory instead of doing it line by line.\n  -f, --fields <fields>               List of fields to process. Defaults to field auto-detection.\n  -v, --default-value <defaultValue>  Default value to use for missing fields.\n  -q, --quote <quote>                 Character(s) to use as quote mark. Defaults to '\"'.\n  -Q, --escaped-quote <escapedQuote>  Character(s) to use as a escaped quote. Defaults to a double `quote`, '\"\"'.\n  -d, --delimiter <delimiter>         Character(s) to use as delimiter. Defaults to ','. (default: \",\")\n  -e, --eol <eol>                     Character(s) to use as End-of-Line for separating rows. Defaults to '\\n'. (default: \"\\n\")\n  -E, --excel-strings                 Wraps string data to force Excel to interpret it as string even if it contains a number.\n  -H, --no-header                     Disable the column name header.\n  -a, --include-empty-rows            Includes empty rows in the resulting CSV output.\n  -b, --with-bom                      Includes BOM character at the beginning of the CSV.\n  -p, --pretty                        Print output as a pretty table. Use only when printing to console.\n  --unwind [paths]                    Creates multiple rows from a single JSON document similar to MongoDB unwind.\n  --unwind-blank                      When unwinding, blank out instead of repeating data. Defaults to false. (default: false)\n  --flatten-objects                   Flatten nested objects. Defaults to false. (default: false)\n  --flatten-arrays                    Flatten nested arrays. Defaults to false. (default: false)\n  --flatten-separator <separator>     Flattened keys separator. Defaults to '.'. (default: \".\")\n  -h, --help                          output usage information\n```\n\nIf no input `-i` is specified the result is expected from to the console standard input.\nIf no output `-o` is specified the result is printed to the console standard output.\nIf no fields `-f` or config `-c` are passed the fields of the first element are used since json2csv CLI process the items one at a time. You can use the `--no-streaming` flag to load the entire JSON in memory and get all the headers. However, keep in mind that this is slower and requires much more memory.\nUse `-p` to show the result as a table in the console.\n\nAny option passed through the config file `-c` will be overriden if a specific flag is passed as well. For example, the fields option of the config will be overriden if the fields flag `-f` is used.\n\n### CLI examples\n\nAll examples use this example [input file](https://github.com/zemirco/json2csv/blob/master/test/fixtures/json/default.json).\n\n#### Input file and specify fields\n\n```sh\n$ json2csv -i input.json -f carModel,price,color\ncarModel,price,color\n\"Audi\",10000,\"blue\"\n\"BMW\",15000,\"red\"\n\"Mercedes\",20000,\"yellow\"\n\"Porsche\",30000,\"green\"\n```\n\n#### Input file, specify fields and use pretty logging\n\n```sh\n$ json2csv -i input.json -f carModel,price,color -p\n```\n\n![Screenshot](https://s3.amazonaws.com/zeMirco/github/json2csv/json2csv-pretty.png)\n\n#### Generating CSV containing only specific fields\n\n```sh\n$ json2csv -i input.json -f carModel,price,color -o out.csv\n$ cat out.csv\ncarModel,price,color\n\"Audi\",10000,\"blue\"\n\"BMW\",15000,\"red\"\n\"Mercedes\",20000,\"yellow\"\n\"Porsche\",30000,\"green\"\n```\n\nSame result will be obtained passing the fields config as a file.\n\n```sh\n$ json2csv -i input.json -c fieldsConfig.json -o out.csv\n```\n\nwhere the file `fieldsConfig.json` contains\n\n```json\n[\n  \"carModel\",\n  \"price\",\n  \"color\"\n]\n```\n\n#### Read input from stdin\n\n```sh\n$ json2csv -f price\n[{\"price\":1000},{\"price\":2000}]\n```\n\nHit <kbd>Enter</kbd> and afterwards <kbd>CTRL</kbd> + <kbd>D</kbd> to end reading from stdin. The terminal should show\n\n```\nprice\n1000\n2000\n```\n\n#### Appending to existing CSV\n\nSometimes you want to add some additional rows with the same columns.\nThis is how you can do that.\n\n```sh\n# Initial creation of csv with headings\n$ json2csv -i test.json -f name,version > test.csv\n# Append additional rows\n$ json2csv -i test.json -f name,version --no-header >> test.csv\n```\n\n## Javascript module\n\n`json2csv` can also be use programatically from you javascript codebase.\n\n### Available Options\n\nThe programatic APIs take a configuration object very equivalent to the CLI options. \n\n- `fields` - Array of Objects/Strings. Defaults to toplevel JSON attributes. See example below.\n- `ndjson` - Only effective on the streaming API. Indicates that data coming through the stream is NDJSON.\n- `transforms` - Array of transforms to be applied to each data item. A transform is simply a function that receives a data item and returns the transformed item.\n- `defaultValue` - String, default value to use when missing data. Defaults to `<empty>` if not specified. (Overridden by `fields[].default`)\n- `quote` - String, quote around cell values and column names. Defaults to `\"` if not specified.\n- `escapedQuote` - String, the value to replace escaped quotes in strings. Defaults to 2x`quotes` (for example `\"\"`) if not specified.\n- `delimiter` - String, delimiter of columns. Defaults to `,` if not specified.\n- `eol` - String, overrides the default OS line ending (i.e. `\\n` on Unix and `\\r\\n` on Windows).\n- `excelStrings` - Boolean, converts string data into normalized Excel style data.\n- `header` - Boolean, determines whether or not CSV file will contain a title column. Defaults to `true` if not specified.\n- `includeEmptyRows` - Boolean, includes empty rows. Defaults to `false`.\n- `withBOM` - Boolean, with BOM character. Defaults to `false`.\n\n### json2csv parser (Synchronous API)\n\n`json2csv` can also be used programatically as a synchronous converter using its `parse` method. \n```js\nconst { Parser } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\n\ntry {\n  const parser = new Parser(opts);\n  const csv = parser.parse(myData);\n  console.log(csv);\n} catch (err) {\n  console.error(err);\n}\n```\n\nyou can also use the convenience method `parse`\n\n```js\nconst { parse } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\n\ntry {\n  const csv = parse(myData, opts);\n  console.log(csv);\n} catch (err) {\n  console.error(err);\n}\n```\n\nBoth of the methods above load the entire JSON in memory and do the whole processing in-memory while blocking Javascript event loop. For that reason is rarely a good reason to use it until your data is very small or your application doesn't do anything else.\n\n### json2csv async parser (Streaming API)\n\nThe synchronous API has the downside of loading the entire JSON array in memory and blocking javascript's event loop while processing the data. This means that your server won't be able to process more request or your UI will become irresponsive while data is being processed. For those reasons, is rarely a good reason to use it unless your data is very small or your application doesn't do anything else.\n\nThe async parser process the data as a non-blocking stream. This approach ensures a consistent memory footprint and avoid blocking javascript's event loop. Thus, it's better suited for large datasets or system with high concurrency. \n\nOne very important difference between the asynchronous and the synchronous APIs is that using the asynchronous API json objects are processed one by one. In practice, this means that only the fields in the first object of the array are automatically detected and other fields are just ignored. To avoid this, it's advisable to ensure that all the objects contain exactly the same fields or provide the list of fields using the `fields` option.\n\nThe async API uses takes a second options arguments that's directly passed to the underlying streams and accept the same options as the standard [Node.js streams](https://nodejs.org/api/stream.html#stream_new_stream_duplex_options).\n\nInstances of `AsyncParser` expose three objects:\n* *input:* Which allows to push more data\n* *processor:* A readable string representing the whole data processing. You can listen to all the standard events of Node.js streams.\n* *transform:* The json2csv transform. See bellow for more details.\n\n```js\nconst { AsyncParser } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\nconst transformOpts = { highWaterMark: 8192 };\n\nconst asyncParser = new AsyncParser(opts, transformOpts);\n\nlet csv = '';\nasyncParser.processor\n  .on('data', chunk => (csv += chunk.toString()))\n  .on('end', () => console.log(csv))\n  .on('error', err => console.error(err));\n  \n// You can also listen for events on the conversion and see how the header or the lines are coming out.\nasyncParser.transform\n  .on('header', header => console.log(header))\n  .on('line', line => console.log(line))\n  .on('error', err => console.log(err));\n\nasyncParser.input.push(data); // This data might come from an HTTP request, etc.\nasyncParser.input.push(null); // Sending `null` to a stream signal that no more data is expected and ends it.\n```\n\n`AsyncParser` also exposes some convenience methods:\n* `fromInput` allows you to set the input stream.\n* `throughTransform` allows you to add transforms to the stream.\n* `toOutput` allows you to set the output stream.\n* `promise` returns a promise that resolves when the stream ends or errors. Takes a boolean parameter to indicate if the resulting CSV should be kept in-memory and be resolved by the promise.\n\n```js\nconst { createReadStream, createWriteStream } = require('fs');\nconst { AsyncParser } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\nconst transformOpts = { highWaterMark: 8192 };\n\n// Using the promise API\nconst input = createReadStream(inputPath, { encoding: 'utf8' });\nconst asyncParser = new JSON2CSVAsyncParser(opts, transformOpts);\nconst parsingProcessor = asyncParser.fromInput(input);\n\nparsingProcessor.promise()\n  .then(csv => console.log(csv))\n  .catch(err => console.error(err));\n\n// Using the promise API just to know when the process finnish\n// but not actually load the CSV in memory\nconst input = createReadStream(inputPath, { encoding: 'utf8' });\nconst output = createWriteStream(outputPath, { encoding: 'utf8' });\nconst asyncParser = new JSON2CSVAsyncParser(opts, transformOpts);\nconst parsingProcessor = asyncParser.fromInput(input).toOutput(output);\n\nparsingProcessor.promise(false).catch(err => console.error(err));\n```\n\nyou can also use the convenience method `parseAsync` which accept both JSON arrays/objects and readable streams and returns a promise.\n\n```js\nconst { parseAsync } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\n\nparseAsync(myData, opts)\n  .then(csv => console.log(csv))\n  .catch(err => console.error(err));\n```\n\n### json2csv transform (Streaming API)\n\njson2csv also exposes the raw stream transform so you can pipe your json content into it. This is the same Transform that `AsyncParser` uses under the hood.\n\n```js\nconst { createReadStream, createWriteStream } = require('fs');\nconst { Transform } = require('json2csv');\n\nconst fields = ['field1', 'field2', 'field3'];\nconst opts = { fields };\nconst transformOpts = { highWaterMark: 16384, encoding: 'utf-8' };\n\nconst input = createReadStream(inputPath, { encoding: 'utf8' });\nconst output = createWriteStream(outputPath, { encoding: 'utf8' });\nconst json2csv = new Transform(opts, transformOpts);\n\nconst processor = input.pipe(json2csv).pipe(output);\n\n// You can also listen for events on the conversion and see how the header or the lines are coming out.\njson2csv\n  .on('header', header => console.log(header))\n  .on('line', line => console.log(line))\n  .on('error', err => console.log(err));\n```\n\nThe stream API can also work on object mode. This is useful when you have an input stream in object mode or if you are getting JSON objects one by one and want to convert them to CSV as they come.\n\n```js\nconst { Transform } = require(\"json2csv\");\nconst { Readable } = require('stream');\n\nconst input = new Readable({ objectMode: true });\ninput._read = () => {};\n// myObjectEmitter is just a fake example representing anything that emit objects.\nmyObjectEmitter.on('object', obj => input.push(obj));\n// Pushing a null close the stream\nmyObjectEmitter.end(() => input.push(null));\n\nconst output = process.stdout;\n\nconst opts = {};\nconst transformOpts = { objectMode: true };\n\nconst json2csv = new Transform(opts, transformOpts);\nconst processor = input.pipe(json2csv).pipe(output);\n```\n\n### Data transforms\n\njson2csv supports data transforms. A transform is simply a function that receives a data item and returns the transformed item.\n\n\n#### Custom transforms\n\n```js\nfunction (item) {\n  // apply tranformations or create new object\n  return transformedItem;\n}\n```\nor using ES6 \n```js\n(item) => {\n  // apply tranformations or create new object\n  return transformedItem;\n}\n```\n\nFor example, let's add a line counter to our CSV, capitalize the car field and change the price to be in Ks (1000s).\n```js\nlet counter = 1;\n(item) => ({ counter: counter++, ...item, car: item.car.toUpperCase(), price: item.price / 1000 });\n```\n\n#### Built-in transforms\n\nThere is a number of built-in transform provider by the library.\n\n```js\nconst { transforms: { unwind, flatten } } = require('json2csv');\n```\n\n##### Unwind\n\nThe unwind transform deconstructs an array field from the input item to output a row for each element. Is's similar to MongoDB's $unwind aggregation.\n\nThe transform needs to be instantiated and takes an options object as arguments containing:\n- `paths` - Array of String, list the paths to the fields to be unwound. It's mandatory and should not be empty.\n- `blankOut` - Boolean, unwind using blank values instead of repeating data. Defaults to `false`.\n\n```js\n// Default\nunwind({ paths: ['fieldToUnwind'] });\n\n// Blanking out repeated data\nunwind({ paths: ['fieldToUnwind'], blankOut: true });\n```\n\n##### Flatten\nFlatten nested javascript objects into a single level object.\n\nThe transform needs to be instantiated and takes an options object as arguments containing:\n- `objects` - Boolean, whether to flatten JSON objects or not. Defaults to `true`.\n- `arrays`- Boolean, whether to flatten Arrays or not. Defaults to `false`.\n- `separator` - String, separator to use between nested JSON keys when flattening a field. Defaults to `.`.\n\n```js\n// Default\nflatten();\n\n// Custom separator '__'\nflatten({ separator: '_' });\n\n// Flatten only arrays\nflatten({ objects: false, arrays: true });\n```\n\n### Javascript module examples\n\n#### Example `fields` option\n```js\n{\n  fields: [\n    // Supports pathname -> pathvalue\n    'simplepath', // equivalent to {value:'simplepath'}\n    'path.to.value' // also equivalent to {value:'path.to.value'}\n\n    // Supports label -> simple path\n    {\n      label: 'some label', // Optional, column will be labeled 'path.to.something' if not defined)\n      value: 'path.to.something', // data.path.to.something\n      default: 'NULL' // default if value is not found (Optional, overrides `defaultValue` for column)\n    },\n\n    // Supports label -> derived value\n    {\n      label: 'some label', // Optional, column will be labeled with the function name or empty if the function is anonymous\n      value: (row, field) => row[field.label].toLowerCase() ||field.default,\n      default: 'NULL' // default if value function returns null or undefined\n    },\n\n    // Supports label -> derived value\n    {\n      value: (row) => row.arrayField.join(',')\n    },\n\n    // Supports label -> derived value\n    {\n      value: (row) => `\"${row.arrayField.join(',')}\"`\n    },\n  ]\n}\n```\n\n#### Example 1\n\n```js\nconst { Parser } = require('json2csv');\n\nconst myCars = [\n  {\n    \"car\": \"Audi\",\n    \"price\": 40000,\n    \"color\": \"blue\"\n  }, {\n    \"car\": \"BMW\",\n    \"price\": 35000,\n    \"color\": \"black\"\n  }, {\n    \"car\": \"Porsche\",\n    \"price\": 60000,\n    \"color\": \"green\"\n  }\n];\n\nconst json2csvParser = new Parser();\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"car\", \"price\", \"color\"\n\"Audi\", 40000, \"blue\"\n\"BMW\", 35000, \"black\"\n\"Porsche\", 60000, \"green\"\n```\n\n#### Example 2\n\nYou can choose which fields to include in the CSV.\n\n```js\nconst { Parser } = require('json2csv');\nconst fields = ['car', 'color'];\n\nconst json2csvParser = new Parser({ fields });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"car\", \"color\"\n\"Audi\", \"blue\"\n\"BMW\", \"black\"\n\"Porsche\", \"green\"\n```\n\n#### Example 3\n\nYou can choose custom column names for the exported file.\n\n```js\nconst { Parser } = require('json2csv');\n\nconst fields = [{\n  label: 'Car Name',\n  value: 'car'\n},{\n  label: 'Price USD',\n  value: 'price'\n}];\n\nconst json2csvParser = new Parser({ fields });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"Car Name\", \"Price USD\"\n\"Audi\", 40000\n\"BMW\", 35000\n\"Porsche\", 60000\n```\n\n#### Example 4\n\nYou can also specify nested properties using dot notation.\n\n```js\nconst { Parser } = require('json2csv');\n\nconst myCars = [\n  {\n    \"car\": { \"make\": \"Audi\", \"model\": \"A3\" },\n    \"price\": 40000,\n    \"color\": \"blue\"\n  }, {\n    \"car\": { \"make\": \"BMW\", \"model\": \"F20\" },\n    \"price\": 35000,\n    \"color\": \"black\"\n  }, {\n    \"car\": { \"make\": \"Porsche\", \"model\": \"9PA AF1\" },\n    \"price\": 60000,\n    \"color\": \"green\"\n  }\n];\n\nconst fields = ['car.make', 'car.model', 'price', 'color'];\n\nconst json2csvParser = new Parser({ fields });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"car.make\", \"car.model\", \"price\", \"color\"\n\"Audi\", \"A3\", 40000, \"blue\"\n\"BMW\", \"F20\", 35000, \"black\"\n\"Porsche\", \"9PA AF1\", 60000, \"green\"\n```\n\n#### Example 5\n\nUse a custom delimiter to create tsv files using the delimiter option:\n\n```js\nconst { Parser } = require('json2csv');\n\nconst json2csvParser = new Parser({ delimiter: '\\t' });\nconst tsv = json2csvParser.parse(myCars);\n\nconsole.log(tsv);\n```\n\nwill output to console\n\n```\n\"car\" \"price\" \"color\"\n\"Audi\"  10000 \"blue\"\n\"BMW\" 15000 \"red\"\n\"Mercedes\"  20000 \"yellow\"\n\"Porsche\" 30000 \"green\"\n```\n\nIf no delimiter is specified, the default `,` is used.\n\n#### Example 6\n\nYou can choose custom quotation marks.\n\n```js\nconst { Parser } = require('json2csv');\n\nconst json2csvParser = new Parser({ quote: '' });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\ncar, price, color\nAudi, 40000, blue\nBMW\", 35000, black\nPorsche\", 60000, green\n```\n\n#### Example 7\n\nYou can unwind arrays similar to MongoDB's $unwind operation using the `unwind` transform.\n\n```js\nconst { Parser, transforms: { unwind } } = require('json2csv');\n\nconst myCars = [\n  {\n    \"carModel\": \"Audi\",\n    \"price\": 0,\n    \"colors\": [\"blue\",\"green\",\"yellow\"]\n  }, {\n    \"carModel\": \"BMW\",\n    \"price\": 15000,\n    \"colors\": [\"red\",\"blue\"]\n  }, {\n    \"carModel\": \"Mercedes\",\n    \"price\": 20000,\n    \"colors\": \"yellow\"\n  }, {\n    \"carModel\": \"Porsche\",\n    \"price\": 30000,\n    \"colors\": [\"green\",\"teal\",\"aqua\"]\n  }\n];\n\nconst fields = ['carModel', 'price', 'colors'];\nconst transforms = [unwind({ paths: ['colors'] })];\n\nconst json2csvParser = new Parser({ fields, transforms });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"carModel\",\"price\",\"colors\"\n\"Audi\",0,\"blue\"\n\"Audi\",0,\"green\"\n\"Audi\",0,\"yellow\"\n\"BMW\",15000,\"red\"\n\"BMW\",15000,\"blue\"\n\"Mercedes\",20000,\"yellow\"\n\"Porsche\",30000,\"green\"\n\"Porsche\",30000,\"teal\"\n\"Porsche\",30000,\"aqua\"\n```\n\n#### Example 8\n\nYou can also unwind arrays multiple times or with nested objects.\n\n```js\nconst { Parser, transforms: { unwind } } = require('json2csv');\n\nconst myCars = [\n  {\n    \"carModel\": \"BMW\",\n    \"price\": 15000,\n    \"items\": [\n      {\n        \"name\": \"airbag\",\n        \"color\": \"white\"\n      }, {\n        \"name\": \"dashboard\",\n        \"color\": \"black\"\n      }\n    ]\n  }, {\n    \"carModel\": \"Porsche\",\n    \"price\": 30000,\n    \"items\": [\n      {\n        \"name\": \"airbag\",\n        \"items\": [\n          {\n            \"position\": \"left\",\n            \"color\": \"white\"\n          }, {\n            \"position\": \"right\",\n            \"color\": \"gray\"\n          }\n        ]\n      }, {\n        \"name\": \"dashboard\",\n        \"items\": [\n          {\n            \"position\": \"left\",\n            \"color\": \"gray\"\n          }, {\n            \"position\": \"right\",\n            \"color\": \"black\"\n          }\n        ]\n      }\n    ]\n  }\n];\n\nconst fields = ['carModel', 'price', 'items.name', 'items.color', 'items.items.position', 'items.items.color'];\nconst transforms = [unwind({ paths: ['items', 'items.items'] })];\nconst json2csvParser = new Parser({ fields, transforms });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"carModel\",\"price\",\"items.name\",\"items.color\",\"items.items.position\",\"items.items.color\"\n\"BMW\",15000,\"airbag\",\"white\",,\n\"BMW\",15000,\"dashboard\",\"black\",,\n\"Porsche\",30000,\"airbag\",,\"left\",\"white\"\n\"Porsche\",30000,\"airbag\",,\"right\",\"gray\"\n\"Porsche\",30000,\"dashboard\",,\"left\",\"gray\"\n\"Porsche\",30000,\"dashboard\",,\"right\",\"black\"\n```\n\n#### Example 9\n\nYou can also unwind arrays blanking the repeated fields.\n\n```js\nconst { Parser, transforms: { unwind }  } = require('json2csv');\n\nconst myCars = [\n  {\n    \"carModel\": \"BMW\",\n    \"price\": 15000,\n    \"items\": [\n      {\n        \"name\": \"airbag\",\n        \"color\": \"white\"\n      }, {\n        \"name\": \"dashboard\",\n        \"color\": \"black\"\n      }\n    ]\n  }, {\n    \"carModel\": \"Porsche\",\n    \"price\": 30000,\n    \"items\": [\n      {\n        \"name\": \"airbag\",\n        \"items\": [\n          {\n            \"position\": \"left\",\n            \"color\": \"white\"\n          }, {\n            \"position\": \"right\",\n            \"color\": \"gray\"\n          }\n        ]\n      }, {\n        \"name\": \"dashboard\",\n        \"items\": [\n          {\n            \"position\": \"left\",\n            \"color\": \"gray\"\n          }, {\n            \"position\": \"right\",\n            \"color\": \"black\"\n          }\n        ]\n      }\n    ]\n  }\n];\n\nconst fields = ['carModel', 'price', 'items.name', 'items.color', 'items.items.position', 'items.items.color'];\nconst transforms = [unwind({ paths: ['items', 'items.items'], blankOut: true })];\n\nconst json2csvParser = new Parser({ fields, transforms });\nconst csv = json2csvParser.parse(myCars);\n\nconsole.log(csv);\n```\n\nwill output to console\n\n```\n\"carModel\",\"price\",\"items.name\",\"items.color\",\"items.items.position\",\"items.items.color\"\n\"BMW\",15000,\"airbag\",\"white\",,\n,,\"dashboard\",\"black\",,\n\"Porsche\",30000,\"airbag\",,\"left\",\"white\"\n,,,,\"right\",\"gray\"\n,,\"dashboard\",,\"left\",\"gray\"\n,,,,\"right\",\"black\"\n```\n\n### Migrations\n\n#### Migrating from 3.X to 4.X\n\nWhat in 3.X used to be\n```js\nconst json2csv = require('json2csv');\nconst csv = json2csv({ data: myData, fields: myFields, unwindPath: paths, ... });\n```\n\nshould be replaced by\n```js\nconst { Parser } = require('json2csv');\nconst json2csvParser = new Parser({ fields: myFields, unwind: paths, ... });\nconst csv = json2csvParser.parse(myData);\n```\n\nor the convenience method\n```js\nconst json2csv = require('json2csv');\nconst csv = json2csv.parse(myData, { fields: myFields, unwind: paths, ... });\n```\n\nPlease note that many of the configuration parameters have been slightly renamed. Please check one by one that all your parameters are correct.\nYou can se the documentation for json2csv 3.11.5 [here](https://github.com/zemirco/json2csv/blob/v3.11.5/README.md).\n\n#### Migrating from 4.X to 5.X\n\nIn the CLI, the config file option, `-c`, used to be a list of fields and now it's expected to be a full configuration object.\n\nThe `stringify` option hass been removed.\n\n`doubleQuote` has been renamed to `escapedQuote`.\n\nThe `unwind` and `flatten` -related options has been moved to their own transforms.\n\nWhat used to be \n```js\nconst { Parser } = require('json2csv');\nconst json2csvParser = new Parser({ unwind: paths, unwindBlank: true, flatten: true, flattenSeparator: '__' });\nconst csv = json2csvParser.parse(myData);\n```\n\nshould be replaced by\n```js\nconst { Parser, transforms: { unwind, flatten } } = require('json2csv');\nconst json2csvParser = new Parser({ transforms: [unwind({ paths, blankOut: true }), flatten('__')] });\nconst csv = json2csvParser.parse(myData);\n```\n\nYou can se the documentation for json2csv v4.X.X [here](https://github.com/zemirco/json2csv/blob/v4/README.md).\n\n## Known Gotchas\n\n### Excel support\n\n#### Avoiding excel autoformatting\n\nExcel tries to automatically detect the format of every field (number, date, string, etc.) regardless of whether the field is quoted or not.\n\nThis might produce few undesired effects with, for example, serial numbers:\n- Large numbers are displayed using scientific notation\n- Leading zeros are stripped.\n\nEnabling the `excelString` option produces an Excel-specific CSV file that forces Excel to interpret string fields as strings. Please note that the CSV will look incorrect if viewing it somewhere else than Excel.\n\n#### Avoiding CSV injection\n\nAs part of Excel automatically format detection, fields regarded as formulas (starting with `=`, `+`, `-` or `@`) are interpreted regardless of whether the field is quoted or not, creating a security risk (see [CSV Injection](https://www.owasp.org/index.php/CSV_Injection).\n\nThis issue has nothing to do with the CSV format, since CSV knows nothing about formulas, but with how Excel parses CSV files.\n\nEnabling the `excelString` option produces an Excel-specific CSV file that forces Excel to interpret string fields as strings. Please note that the CSV will look incorrect if viewing it somewhere else than Excel.\n\n#### Preserving new lines\n\nExcel only recognizes `\\r\\n` as valid new line inside a cell.\n\n#### Unicode Support\n\nExcel can display Unicode correctly (just setting the `withBOM` option to true). However, Excel can't save unicode so, if you perform any changes to the CSV and save it from Excel, the Unicode characters will not be displayed correctly.\n\n\n### PowerShell escaping\n\nPowerShell do some estrange double quote escaping escaping which results on each line of the CSV missing the first and last quote if outputting the result directly to stdout. Instead of that, it's advisable that you write the result directly to a file.\n\n## Building\n\njson2csv is packaged using `rollup`. You can generate the packages running:\n\n```sh\nnpm run build\n```\nwhich generates 3 files under the `dist folder`:\n\n* `json2csv.umd.js` UMD module transpiled to ES5\n* `json2csv.esm.js` ES5 module (import/export)\n* `json2csv.cjs.js` CommonJS module\n\nWhen you use packaging tools like webpack and such, they know which version to use depending on your configuration.\n\n## Testing\n\nRun the folowing command to check the code style.\n\n```sh\n$ npm run lint\n```\n\nRun the following command to run the tests and return coverage\n\n```sh\n$ npm run test-with-coverage\n```\n\n## Contributors\n\nAfter you clone the repository you just need to install the required packages for development by runnning following command under json2csv dir.\n\n```sh\n$ npm install\n```\n\nBefore making any pull request please ensure sure that your code is formatted, test are passing and test coverage haven't decreased. (See [Testing](#testing))\n\n## License\n\nSee [LICENSE.md].\n\n[npm-badge]: https://badge.fury.io/js/json2csv.svg\n[npm-badge-url]: http://badge.fury.io/js/json2csv\n[travis-badge]: https://travis-ci.org/zemirco/json2csv.svg\n[travis-badge-url]: https://travis-ci.org/zemirco/json2csv\n[coveralls-badge]: https://coveralls.io/repos/zemirco/json2csv/badge.svg?branch=master\n[coveralls-badge-url]: https://coveralls.io/r/zemirco/json2csv?branch=master\n[CHANGELOG]: https://github.com/zemirco/json2csv/blob/master/CHANGELOG.md\n[LICENSE.md]: https://github.com/zemirco/json2csv/blob/master/LICENSE.md\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/zemirco/json2csv.git"
  },
  "scripts": {
    "before:publish": "npm test && npm run build && npm run deploy:docs",
    "build": "rollup -c",
    "coveralls": "nyc report --reporter=text-lcov | coveralls",
    "deploy:docs": "docpress b && gh-pages -d _docpress",
    "dev": "rollup -c -w",
    "lint": "eslint bin lib test",
    "prepublish": "in-publish && npm run before:publish || not-in-publish",
    "release": "standard-version",
    "test": "node test | tap-spec",
    "test-with-coverage": "nyc --reporter=text node test | tap-spec"
  },
  "version": "5.0.6",
  "volta": {
    "node": "10.19.0"
  },
  "warnings": [
    {
      "code": "ENOTSUP",
      "required": {
        "node": ">= 10",
        "npm": ">= 6.13.0"
      },
      "pkgid": "json2csv@5.0.6"
    },
    {
      "code": "ENOTSUP",
      "required": {
        "node": ">= 10",
        "npm": ">= 6.13.0"
      },
      "pkgid": "json2csv@5.0.6"
    }
  ]
}
